import java.io.IOException;
import java.util.*;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


public class WordCount {
	/* Ref: http://hadoop.apache.org/docs/r2.7.1/api/org/apache/hadoop/mapreduce/Mapper.html
	 * 
	 * Maps are the individual tasks which transform input records into a intermediate records. 
	 * The transformed intermediate records need not be of the same type as the input records.
	 * A given input pair may map to zero or many output pairs.
	 *  
	 * The Hadoop Map-Reduce framework spawns one map task for each InputSplit generated by the 
	 * InputFormat for the job. Mapper implementations can access the Configuration for the job 
	 * via the JobContext.getConfiguration().
	 * 
	 * The framework first calls setup(org.apache.hadoop.mapreduce.Mapper.Context), 
	 * followed by map(Object, Object, org.apache.hadoop.mapreduce.Mapper.Context) for each 
	 * key/value pair in the InputSplit. 
	 * Finally cleanup(org.apache.hadoop.mapreduce.Mapper.Context) is called.
	 */
	 
	 
	// org.apache.hadoop.mapreduce.Mapper<KEYIN,VALUEIN,KEYOUT,VALUEOUT>
    public static class WordCountMap extends Mapper<Object, Text, Text, IntWritable> {
        @Override
		public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
			String line = value.toString();
			StringTokenizer tokenizer = new StringTokenizer(line);
			while (tokenizer.hasMoreTokens()) {
				String nextToken = tokenizer.nextToken();
				context.write(new Text(nextToken), new IntWritable(1));
			}
		}
	}
    
    
    /* Ref: http://hadoop.apache.org/docs/r2.7.1/api/org/apache/hadoop/mapreduce/Reducer.html
     * 
     * Reduces a set of intermediate values which share a key to a smaller set of values.
     * 
     * Reducer has 3 primary phases:
     * 1. Shuffle: The Reducer copies the sorted output from each Mapper using HTTP across the network.
     * 2. Sort: The framework merge sorts Reducer inputs by keys (since different Mappers may have output 
     * 	  the same key). The shuffle and sort phases occur simultaneously i.e. while outputs are being 
     *    fetched they are merged. Then SecondarySort.
     * 3. Reduce: In this phase the reduce(Object, Iterable, org.apache.hadoop.mapreduce.Reducer.Context) 
     * 	  method is called for each <key, (collection of values)> in the sorted inputs.
	 *	  The output of the reduce task is typically written to a RecordWriter via 
	 *    TaskInputOutputContext.write(Object, Object). (Output is not-sorted).
     */
    
    // org.apache.hadoop.mapreduce.Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT>
    public static class WordCountReduce extends Reducer<Text, IntWritable, Text, IntWritable> {
		@Override
        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
			int sum = 0;
			for (IntWritable val : values) {
				sum += val.get();
			}
			context.write(key, new IntWritable(sum));
		}
	}

	public static void main(String[] args) throws Exception {

        Job job = Job.getInstance(new Configuration(), "wordcount");
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        job.setMapperClass(WordCountMap.class);
        job.setReducerClass(WordCountReduce.class);

        FileInputFormat.setInputPaths(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        job.setJarByClass(WordCount.class);
		System.exit(job.waitForCompletion(true) ? 0 : 1);
	}
}